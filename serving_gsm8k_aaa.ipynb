{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBILE_DEVICES=0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from vllm import LLM, SamplingParams\n",
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "from torchtune import config\n",
    "from torchtune.config._utils import _get_component_from_path\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from metaflow import Run\n",
    "from utils import fetch_and_load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to download the model on local disk?\n",
    "checkpoint_cache=\"./trained_models\"\n",
    "\n",
    "# Properties of upstream Metaflow run.\n",
    "artifact_name = \"model_ref\"\n",
    "\n",
    "# Reward server version selection.\n",
    "version = 'v1'\n",
    "if version == 'v0':\n",
    "    from rewards_gsm8k_aaa_v0 import RewardServer \n",
    "    reward_tag = 'reward:gsm8k_aaa_v0'\n",
    "elif version == 'v1':\n",
    "    from rewards_gsm8k_aaa_v1 import RewardServer\n",
    "    reward_tag = 'reward:gsm8k_aaa_v1'\n",
    "    \n",
    "# Properties of torchtune / finetuning run.\n",
    "dataset_component = 'torchtune.dev.grpo.gsm8k.gsm8k_dataset'\n",
    "dataset_partition = '3-5/100'\n",
    "\n",
    "# Inference server properties\n",
    "n_gpu = len(os.environ['CUDA_VISIBILE_DEVICES'].split(','))\n",
    "batch_size = 2\n",
    "grpo_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Fetch by tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tag = 'model:meta-llama/Llama-3.2-3B-Instruct'\n",
    "flow_name = 'TorchtuneGRPOSingleNode'\n",
    "\n",
    "# Fetch model weights. \n",
    "# model_dir can be consumed by vLLM, or another inference server constructor.\n",
    "model_dir = fetch_and_load_weights(\n",
    "    model_tag = model_tag,\n",
    "    reward_tag = reward_tag,\n",
    "    flow_name = flow_name,\n",
    "    checkpoint_cache=checkpoint_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Fetch with specific run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Run('TorchtuneGRPOSingleNode/9195') # NOTE: this particular run id is aaa_v1, a spectacularly dumb trial design fail. \n",
    "model_dir = fetch_and_load_weights(\n",
    "    run=run,\n",
    "    reward_tag = reward_tag,\n",
    "    checkpoint_cache=checkpoint_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights into memory. \n",
    "# vLLM optimizes layout automatically.\n",
    "llm = LLM(\n",
    "    model=model_dir, \n",
    "    task=\"generate\", \n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=n_gpu,\n",
    "    dtype='bfloat16'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do inference, unrolling a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup torchtune dependencies.\n",
    "world_size = n_gpu\n",
    "rank = 0\n",
    "cfg_dataset = DictConfig({'_component_': dataset_component, 'partition': dataset_partition})\n",
    "cfg_tokenizer = DictConfig({\n",
    "    '_component_': 'torchtune.models.llama3.llama3_tokenizer',\n",
    "    'path': os.path.join(model_dir, 'original/tokenizer.model'),\n",
    "    'max_seq_len': 'null'\n",
    "})\n",
    "collate_fn = 'torchtune.dev.grpo.data.padded_collate_rl'\n",
    "\n",
    "tokenizer = config.instantiate(cfg_tokenizer)\n",
    "ds = config.instantiate(cfg_dataset, tokenizer)\n",
    "collate_fn = _get_component_from_path(collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = DistributedSampler(\n",
    "    ds,\n",
    "    num_replicas=world_size,\n",
    "    rank=rank,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=ds,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler,\n",
    "    # dropping last avoids shape issues with compile + flex attention\n",
    "    drop_last=True,\n",
    "    collate_fn=(\n",
    "        partial(\n",
    "            collate_fn,\n",
    "            padding_idx=tokenizer.pad_id,\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View `batch_size=2` sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dataloader._get_iterator())\n",
    "tokens = batch[\"tokens\"]         # tokenized prompts\n",
    "answers = batch[\"answers\"]       # untokenized answers\n",
    "tokens = tokens #.to(self._device) # [batch_size x num_tokens_per_prompt]\n",
    "tokens_ls = tokens.tolist()\n",
    "out = []\n",
    "_prompts = []\n",
    "_answers = []\n",
    "for i in range(tokens.shape[0]):\n",
    "    prompt = tokenizer.decode(tokens_ls[i])\n",
    "    _prompts.extend([prompt] * grpo_size) \n",
    "    answer = answers[i]\n",
    "    _answers.extend([answer] * grpo_size)\n",
    "    out.append(prompt+'\\n' + '-'*24 + '\\n' + 'GROUND_TRUTH_ANSWER: ' + answer)\n",
    "sep =  '\\n' + \"-\"*24 + '\\n'\n",
    "\n",
    "formatted_output = sep.join(out).replace('\\n', '<br>')\n",
    "display(HTML(f\"<div style='max-width:500px'>{formatted_output}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8, \n",
    "    top_p=0.95,\n",
    "    max_tokens=512\n",
    ")\n",
    "output = llm.generate(_prompts, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token_ids = [\n",
    "    128001,\n",
    "    128009,\n",
    "    128008\n",
    "]\n",
    "pad_id = 128004\n",
    "max_tokens = 512\n",
    "\n",
    "data = []\n",
    "for o in output:\n",
    "    out_tokens = list(o.outputs[0].token_ids)\n",
    "    if len(out_tokens) < max_tokens:\n",
    "        out_tokens += [pad_id] * (max_tokens - len(out_tokens))\n",
    "    data.append(out_tokens)\n",
    "responses=torch.tensor(data, dtype=torch.int32).reshape(batch_size, grpo_size, max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pluggable Reward Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_server = RewardServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, successes, details = reward_server.batch_shaped_correctness_reward(\n",
    "    tokenizer, responses, answers*2, details_report=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages = (rewards - rewards.mean(1, keepdim=True)) / (\n",
    "    rewards.std(1, keepdim=True) + 1e-4\n",
    ")\n",
    "# advantages = advantages.reshape(batch_size * grpo_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.shape, advantages.shape, responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\n",
    "    reward_server.display_responses(\n",
    "        responses=responses,\n",
    "        tokenizer=tokenizer, \n",
    "        grpo_size=grpo_size, \n",
    "        advantages=advantages, \n",
    "        rewards=rewards, \n",
    "        successes=successes,\n",
    "        details=details,\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
